{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, NVARCHAR, Text ,DateTime , text , exc, inspect\n",
    "from sqlalchemy.types import NVARCHAR, Text\n",
    "import pandas as pd\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Una empresa llamada DBU que vende equipos deportivos al aire libre, \\\n",
    "tiene muchas ubicaciones diferentes y ha estado registrando las ventas de diferentes ubicaciones en varios productos.\\\n",
    "Quiere saber cuÃ¡les son sus mejores productos y vendedores para mejorar su rendimiento general_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base:str = \"https://raw.githubusercontent.com/CoderContenidos/Data.Engineering/main/Semana%207/Tablas/\"\n",
    "tables:list[str]= [\n",
    "    \"countryregioncurrency.csv\",\n",
    "    \"currencyrate.csv\",\n",
    "    \"product.csv\",\n",
    "    \"productcategory.csv\",\n",
    "    \"productdescription.csv\",\n",
    "    \"productmodelproductdescriptionculture.csv\",\n",
    "    \"productreview.csv\",\n",
    "    \"productsubcategory.csv\",\n",
    "    \"salesorderdetail.csv\",\n",
    "    \"salesorderheader.csv\",\n",
    "    \"salesperson.csv\",\n",
    "    \"salesterritory.csv\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema:str = \"andru_ocatorres_coderhouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes con SQL Alchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [SQLAlchemy Documentation]( https://docs.sqlalchemy.org/en/20/intro.html#installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando las librerias necesarias para hacer la conexion a la base\n",
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un objeto url para conectar con la warehouse\n",
    "username = os.getenv('REDSHIFT_USERNAME')\n",
    "password = os.getenv('REDSHIFT_PASSWORD')\n",
    "host = os.getenv('REDSHIFT_HOST')\n",
    "port = os.getenv('REDSHIFT_PORT', '5439')\n",
    "dbname = os.getenv('REDSHIFT_DBNAME')\n",
    "\n",
    "# Construct the connection URL\n",
    "connection_url = f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{dbname}\"\n",
    "db_engine = create_engine(connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexion creada\n"
     ]
    }
   ],
   "source": [
    "# verificamos la conexion\n",
    "try:\n",
    "    with db_engine.connect() as connection:\n",
    "        print(\"Conexion creada\")\n",
    "except Exception as e:\n",
    "    print(f\"Conexion fallida: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendacion de los procesos\n",
    "- Estructurar los datos \n",
    "- Descarga de los datos en parquet\n",
    "- Lectura de las tablas por medio de un dataframe\n",
    "- Transformacion si es necesaria\n",
    "- Carga en las tablas generadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si es que se requiere descargar en un formato en particular\n",
    "\n",
    "def merge_chunk_data(csv_file, chunksize) -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for _ in pd.read_csv(csv_file, chunksize=chunksize):\n",
    "        data = pd.concat([data, _], ignore_index=True)\n",
    "\n",
    "    return data\n",
    "    \n",
    "\n",
    "def store_data_to_parquet(paths: list[str], chunksize: int) -> dict:\n",
    "    folder_store: str = \"./data_store\"\n",
    "\n",
    "    for csv_file in paths:\n",
    "        table_name:str = csv_file.split(\".\")[0]\n",
    "        parquet_file: str =  f\"{folder_store}/{table_name}.parquet\"\n",
    "        print(f\"{csv_file} will be read it and download\")\n",
    "        data = merge_chunk_data(csv_file, chunksize)\n",
    "        data.to_parquet(parquet_file, engine='fastparquet') \n",
    "        print(f\"file: {parquet_file} successful created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_table(**kwargs):\n",
    "    base_path = kwargs[\"base_path\"]\n",
    "    csv_file = kwargs[\"csv\"]\n",
    "    engine = kwargs[\"engine\"]\n",
    "\n",
    "    table_name = csv_file.split(\".\")[0]\n",
    "\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(base_path + csv_file, chunksize=10)\n",
    "        df =  next(chunk_iterator)\n",
    "\n",
    "        schema = \"andru_ocatorres_coderhouse\"\n",
    "        metadata = MetaData(schema=schema)\n",
    "\n",
    "        columns = []\n",
    "        \n",
    "        for col_name, col_type in zip(df.columns, df.dtypes):\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                columns.append(Column(col_name, Integer))\n",
    "            elif pd.api.types.is_float_dtype(col_type):\n",
    "                columns.append(Column(col_name, String))  # Use Float for float columns\n",
    "            elif pd.api.types.is_string_dtype(col_type):\n",
    "                columns.append(Column(col_name, NVARCHAR (length=60000)))  # Use Text for string columns\n",
    "            elif pd.api.types.is_datetime64_any_dtype(col_type):\n",
    "                columns.append(Column(col_name, DateTime))  \n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported dtype: {col_type}\")\n",
    "            \n",
    "        inspector = inspect(db_engine)\n",
    "        table_exists = inspector.has_table(table_name, schema=schema)\n",
    "\n",
    "        if table_exists:\n",
    "            print(f\"Table: {table_name} exists, so let's drop it\")\n",
    "\n",
    "            try:\n",
    "                # Use raw SQL to drop the table to ensure it's dropped from the correct schema\n",
    "                with db_engine.connect() as connection:\n",
    "                    connection.execute(f'DROP TABLE IF EXISTS \"{schema}\".\"{table_name}\"')\n",
    "                print(f\"Table {table_name} dropped successfully.\")\n",
    "\n",
    "            except exc.SQLAlchemyError as e:\n",
    "                print(f\"Error occurred while dropping the table: {e}\")\n",
    "        \n",
    "\n",
    "        table = Table(table_name, metadata, *columns)\n",
    "        metadata.create_all(db_engine)\n",
    "        print(f\"Table: {table_name} created!\")\n",
    "    \n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload table {table_name}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para la carga de cada dataset\n",
    "def upload_csv(**kwargs):\n",
    "    base_path = kwargs[\"base_path\"]\n",
    "    csv_file = kwargs[\"csv\"]\n",
    "    engine = kwargs[\"engine\"]\n",
    "    chunksize = kwargs[\"chunksize\"] \n",
    "\n",
    "    table_name = csv_file.split(\".\")[0]\n",
    "    print(f\"CSV: {csv_file} will be uploaded using chunks of: {chunksize}\")\n",
    "    \n",
    "    chunk_iterator = pd.read_csv(base_path + csv_file, chunksize=chunksize)\n",
    "\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk.to_sql(table_name, engine, if_exists='append', index=False, method='multi')\n",
    "        print(f\"Uploaded chunk with {len(chunk)} records\")\n",
    "\n",
    "    \n",
    "    print(f\"Table {table_name} has been successfully populated\")\n",
    "    print(f\"{'*'*100}\")\n",
    "    \n",
    "\n",
    "    \n",
    "# function para crear views areas funcionales\n",
    "\n",
    "\n",
    "\n",
    "def upload_views(**kwargs):\n",
    "    engine = kwargs[\"engine\"]\n",
    "    queries = kwargs[\"queries\"]\n",
    "    schema = kwargs[\"schema\"]\n",
    "    \n",
    "    # Transformation\n",
    "    queries_to_view = {\n",
    "        name: f\"CREATE OR REPLACE VIEW {schema}.{name} AS\\n{query}\"\n",
    "        for name, query in queries.items()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            for name, view_query in queries_to_view.items():\n",
    "                connection.execute(text(view_query).execution_options(autocommit=True))\n",
    "                print(f\"View '{name}' has been created successfully\")\n",
    "\n",
    "        print(\"Transaction committed successfully\")\n",
    "        return 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create view: {e}\")\n",
    "        # Rollback the transaction in case of error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[create_table(base_path=url_base,csv=table,engine=db_engine) for table in tables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[upload_csv(base_path=url_base,chunksize=100_000,csv=table,engine=db_engine) for table in tables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTAS A RESPONDER\n",
    "* _MEJORES PRODUCTOS -> TOP PRODUCTOS EN REVIEW | TOP PRODUCTOS MAS VENDIDOS_\n",
    "* _VENDEDORES CON PEOR VENTA -> TOP N PEORES VENDEDORES_\n",
    "* _Encontrar los cinco vendedores con mejor desempeÃ±o usando la columna salesytd (Sales, year-to-date). (Solo necesitamos conocer el businessentityid de cada vendedor, ya que esto identifica de forma Ãºnica a cada uno)._\n",
    "* _Usando salesorderheader, buscar los 5 mejores vendedores que hicieron la mayor cantidad de ventas en el aÃ±o mÃ¡s reciente (2014). (Hay una columna llamada subtotal; usarla). Las ventas que no tienen un vendedor asociado deben excluirse de los cÃ¡lculos y producciÃ³n final. Se deben incluir todos los pedidos que se realizaron dentro del aÃ±o calendario 2014.\n",
    "Pista: Pueden usar la sintaxis '1970-01-01' para generar un punto de comparaciÃ³n en el tiempo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_worst_sellerstr = \"\"\"\n",
    "\n",
    "\tSELECT \n",
    "\t\tsalespersonid \n",
    "\t,\tROUND(CAST(SUM(totaldue) AS NUMERIC),2) AS TOTAL_AMOUNT_SELLED\n",
    "\t,\tCOUNT(1) AS AMOUNT_OF_SALES\n",
    "\tFROM salesorderheader  \n",
    "\tGROUP BY salespersonid \n",
    "\tORDER BY \n",
    "\t\tTOTAL_AMOUNT_SELLED\n",
    "\tLIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "sql_top_10_best_sells:str = \"\"\"\n",
    "WITH TOP_BEST_SELLS AS\n",
    "(\tSELECT \n",
    "\t\tS.productid\n",
    "\t,\tCOUNT(1) AS PRODUCT_TOTAL\n",
    "\tFROM salesorderdetail  S\n",
    "\tGROUP BY \n",
    "\t\tS.productid\n",
    "),\n",
    "AGG_TABLE as (\n",
    "SELECT \n",
    "\tproductid\n",
    ",\tSUBSTRING(P.productmodelid, 1, CHARINDEX('.', P.productmodelid) - 1)::INTEGER as productmodelid\n",
    ",\tP.name\n",
    ",\tPRODUCT_TOTAL\n",
    "FROM TOP_BEST_SELLS\n",
    "LEFT JOIN \n",
    "\tproduct AS P\n",
    "\tUSING(productid)\n",
    "ORDER BY PRODUCT_TOTAL DESC\n",
    "LIMIT 10\n",
    ")\n",
    "SELECT \n",
    "\tAG.name\n",
    ",\tPD.description\n",
    ",\tAG.PRODUCT_TOTAL\n",
    "FROM productdescription PD\n",
    "INNER JOIN productmodelproductdescriptionculture AS PC USING(productdescriptionid)\n",
    "INNER JOIN AGG_TABLE AS AG USING(productmodelid)\n",
    "WHERE PC.cultureid = 'en' ;\n",
    "\"\"\"\n",
    "\n",
    "sql_best_reviews:str = \"\"\"\n",
    "WITH producto_rating AS (\n",
    "         SELECT pv.productid,\n",
    "         \tSUBSTRING(p.productmodelid, 1, CHARINDEX('.', p.productmodelid) - 1)::INTEGER as productmodelid,\n",
    "            p.name,\n",
    "            avg(pv.rating) AS avg_rating,\n",
    "            count(1) AS total_reviews\n",
    "           FROM productreview pv\n",
    "             LEFT JOIN product p USING (productid)\n",
    "          GROUP BY pv.productid, p.name, p.productmodelid\n",
    "          ORDER BY (avg(pv.rating)) DESC\n",
    "        )\n",
    " SELECT pr.name,\n",
    "    pd.description,\n",
    "    pr.avg_rating,\n",
    "    pr.total_reviews\n",
    "   FROM productdescription pd\n",
    "     JOIN productmodelproductdescriptionculture pc USING (productdescriptionid)\n",
    "     JOIN producto_rating pr USING (productmodelid)\n",
    "  WHERE pc.cultureid = 'en'::text;\"\"\"\n",
    "\n",
    "\n",
    "sql_best_sales_person:str = \"\"\"\n",
    "SELECT \n",
    "   \tbusinessentityid\n",
    ",\tsalesytd\n",
    "FROM salesperson\n",
    "ORDER BY salesytd DESC\n",
    "LIMIT 5 \n",
    "\"\"\"\n",
    "sql_best_sales_person_2014:str = \"\"\"\n",
    "SELECT \n",
    "    salespersonid,\n",
    "    ROUND(SUM(subtotal)::NUMERIC, 2) AS total_sales\n",
    "FROM \n",
    "    salesorderheader\n",
    "WHERE \n",
    "    orderdate >= '2014-01-01'\n",
    "    AND salespersonid IS NOT NULL\n",
    "GROUP BY \n",
    "    salespersonid\n",
    "ORDER BY \n",
    "    total_sales DESC\n",
    "LIMIT 5;\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "queries_to_view = {    \n",
    "\t\"best_sales_person_2014_view\":sql_best_sales_person_2014,\n",
    "    \"best_sales_person_view\":sql_best_sales_person,\n",
    "    \"best_reviews_view\":sql_best_reviews,\n",
    "    \"best_sells_view\":sql_top_10_best_sells,\n",
    "    \"worst_sellers_view\" : sql_worst_sellerstr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar carga de vistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 'best_sales_person_2014_view' has been created successfully\n",
      "View 'best_sales_person_view' has been created successfully\n",
      "View 'best_reviews_view' has been created successfully\n",
      "View 'best_sells_view' has been created successfully\n",
      "View 'worst_sellers_view' has been created successfully\n",
      "Transaction committed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_views(engine=db_engine,queries=queries_to_view, schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes con Psycopg - Linux(Psycopg-binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Psycopg Documentation](https://www.psycopg.org/psycopg3/docs/basic/install.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera un objeto connector\n",
    "conn = pg.connect(\n",
    "        user = os.getenv('REDSHIFT_USERNAME')\n",
    "    ,   password = os.getenv('REDSHIFT_PASSWORD')\n",
    "    ,   host = os.getenv('REDSHIFT_HOST')\n",
    "    ,   port = os.getenv('REDSHIFT_PORT', '5439')\n",
    "    ,   database = os.getenv('REDSHIFT_DBNAME')\n",
    ")\n",
    "\n",
    "schema = \"andru_ocatorres_coderhouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_creator(**kwargs):\n",
    "    connection = kwargs[\"connection\"]\n",
    "    queries = kwargs[\"queries\"]\n",
    "    schema = kwargs[\"schema\"] \n",
    "\n",
    "    try:\n",
    "        # Ensure auto-commit mode is enabled\n",
    "        connection.autocommit = True\n",
    "        \n",
    "        with connection.cursor() as cursor:\n",
    "            queries_to_view = {\n",
    "                name: f\"CREATE OR REPLACE VIEW {schema}.{name} AS\\n{query}\"\n",
    "                for name, query in queries.items()\n",
    "            }\n",
    "\n",
    "            for name, view_query in queries_to_view.items():\n",
    "                cursor.execute(view_query)\n",
    "                print(f\"View '{name}' has been created\")\n",
    "            \n",
    "        # Explicitly commit the transaction\n",
    "        connection.commit()\n",
    "        print(\"Transaction committed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error '{e}' occurred\")\n",
    "        # Rollback the transaction\n",
    "        connection.rollback() \n",
    "\n",
    "    finally:\n",
    "        connection.close()\n",
    "        print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 'best_sales_person_2014_view' has been created\n",
      "View 'best_sales_person_view' has been created\n",
      "View 'best_reviews_view' has been created\n",
      "View 'best_sells_view' has been created\n",
      "View 'worst_sellers_view' has been created\n",
      "Transaction committed successfully\n",
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "view_creator(connection=conn,queries=queries_to_view, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## psycopg2 vs SQLAlchemy\n",
    "\n",
    "| Feature         | psycopg2                                    | SQLAlchemy                                          |\n",
    "|-----------------|---------------------------------------------|-----------------------------------------------------|\n",
    "| Type            | Database adapter                            | ORM library and SQL toolkit                        |\n",
    "| Purpose         | Interact with PostgreSQL databases         | Interact with multiple database engines             |\n",
    "| Functionality   | Low-level interface for executing SQL commands, managing connections, handling transactions | ORM for mapping Python objects to database tables, SQL toolkit for query building |\n",
    "| Level of Abstraction | Low-level, requires writing SQL queries directly | High-level, provides abstraction over SQL queries, supports ORM |\n",
    "| Performance     | Known for efficiency and performance      | Offers flexibility and abstraction, may have slightly more overhead |\n",
    "| Suitability     | Developers comfortable with SQL, need direct control over PostgreSQL interactions | Developers preferring higher-level abstraction, multiple database support |\n",
    "| Learning Curve  | Easier for SQL-savvy developers            | Steeper due to ORM and higher-level abstraction    |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
